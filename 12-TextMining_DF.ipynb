{
 "metadata": {
  "name": "",
  "signature": "sha256:b79b5768303773d7384df36d2bccfae891ef1becd7ac1cfa2c884e969609bd3c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Text Feature Extraction for Classification\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from Olivier Grisel's tutorial)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<style>\n",
      "div.input {\n",
      "    width: 105ex; /* about 80 chars + buffer */\n",
      "}\n",
      "div.text_cell {\n",
      "    width: 105ex; /* instead of 100%, */\n",
      "}\n",
      "div.text_cell_render {\n",
      "    /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/\n",
      "    font-family: \"Charis SIL\", serif !important; /* Make non-code text serif. */\n",
      "    line-height: 145% !important; /* added for some line spacing of text. */\n",
      "    width: 105ex !important; /* instead of 'inherit' for shorter lines */\n",
      "}\n",
      "/* Set the size of the headers */\n",
      "div.text_cell_render h1 {\n",
      "    font-size: 18pt;\n",
      "}\n",
      "div.text_cell_render h2 {\n",
      "    font-size: 14pt;\n",
      "}\n",
      ".CodeMirror {\n",
      "     font-family: Consolas, monospace;\n",
      "}\n",
      "</style>\n",
      "\n",
      "Outline of this section:\n",
      "\n",
      "- Turn a corpus of text documents into **feature vectors** using a **Bag of Words** representation,\n",
      "- Train a simple text classifier on the feature vectors,\n",
      "- Wrap the vectorizer and the classifier with a **pipeline**,\n",
      "- Cross-validation and **model selection** on the pipeline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from seaborn import plt\n",
      "\n",
      "# Some nice default configuration for plots\n",
      "plt.rcParams['figure.figsize'] = 10, 7.5\n",
      "plt.rcParams['axes.grid'] = True\n",
      "plt.gray()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<matplotlib.figure.Figure at 0x365eeb8>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Check that you have the datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run fetch_data.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating datasets folder: C:\\learnpython\\Data Science\\DAT_SF_12\\datasets\n",
        "Checking availability of the 20 newsgroups dataset\n",
        "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n",
        "Decompressing C:\\learnpython\\Data Science\\DAT_SF_12\\datasets\\20news-bydate.tar.gz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Checking that the 20 newsgroups files exist..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "=> Success!\n",
        "Checking availability of the titanic dataset\n",
        "Downloading titanic data from https://dl.dropboxusercontent.com/u/5743203/data/titanic/titanic_train.csv\n",
        "=> Success!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -lh datasets/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "'ls' is not recognized as an internal or external command,\n",
        "operable program or batch file.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Text Classification in 20 lines of Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's start by implementing a canonical text classification example:\n",
      "\n",
      "- The 20 newsgroups dataset: around 18000 text posts from 20 newsgroups forums\n",
      "- Bag of Words features extraction with TF-IDF weighting\n",
      "- Naive Bayes classifier or Linear Support Vector Machine for the classifier itself"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_files\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "# Load the text data\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'talk.religion.misc',\n",
      "    'comp.graphics',\n",
      "    'sci.space',\n",
      "]\n",
      "twenty_train_small = load_files('datasets/20news-bydate-train/',\n",
      "    categories=categories, encoding='latin-1')\n",
      "twenty_test_small = load_files('datasets/20news-bydate-test/',\n",
      "    categories=categories, encoding='latin-1')\n",
      "\n",
      "# Turn the text documents into vectors of word frequencies\n",
      "vectorizer = TfidfVectorizer(min_df=2)\n",
      "X_train = vectorizer.fit_transform(twenty_train_small.data)\n",
      "y_train = twenty_train_small.target\n",
      "\n",
      "# Fit a classifier on the training set\n",
      "classifier = MultinomialNB().fit(X_train, y_train)\n",
      "print(\"Training score: {0:.1f}%\".format(\n",
      "    classifier.score(X_train, y_train) * 100))\n",
      "\n",
      "# Evaluate the classifier on the testing set\n",
      "X_test = vectorizer.transform(twenty_test_small.data)\n",
      "y_test = twenty_test_small.target\n",
      "print(\"Testing score: {0:.1f}%\".format(\n",
      "    classifier.score(X_test, y_test) * 100))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training score: 95.1%\n",
        "Testing score: 85.1%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "[Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
      "--------------------------------------------------------\n",
      "`MultinomialNB` implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
      "\n",
      "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
      "\n",
      "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
      "\n",
      "where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
      "\n",
      "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's now decompose what we just did to understand and customize each step."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Loading the Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's explore the dataset loading utility without passing a list of categories: in this case we load the full 20 newsgroups dataset in memory. The source website for the 20 newsgroups already provides a date-based train / test split that is made available using the `subset` keyword argument: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls datasets/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "'ls' is not recognized as an internal or external command,\n",
        "operable program or batch file.\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh datasets/20news-bydate-train"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Invalid switch - \"20news-bydate-train\".\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh datasets/20news-bydate-train/alt.atheism/ | head -n27"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 4480\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795    12K Mar 18  2003 49960\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795    31K Mar 18  2003 51060\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   4.0K Mar 18  2003 51119\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.6K Mar 18  2003 51120\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   773B Mar 18  2003 51121\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   4.8K Mar 18  2003 51122\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   618B Mar 18  2003 51123\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.4K Mar 18  2003 51124\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   2.7K Mar 18  2003 51125\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   427B Mar 18  2003 51126\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   742B Mar 18  2003 51127\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   650B Mar 18  2003 51128\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.3K Mar 18  2003 51130\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   2.3K Mar 18  2003 51131\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   2.6K Mar 18  2003 51132\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.5K Mar 18  2003 51133\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.2K Mar 18  2003 51134\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.6K Mar 18  2003 51135\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   2.1K Mar 18  2003 51136\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.3K Mar 18  2003 51139\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   409B Mar 18  2003 51140\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   940B Mar 18  2003 51141\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   9.0K Mar 18  2003 51142\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   632B Mar 18  2003 51143\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   1.2K Mar 18  2003 51144\r\n",
        "-rw-r--r--@ 1 alessandro.gagliardi  1260538795   609B Mar 18  2003 51145\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The `load_files` function can load text files from a 2 levels folder structure assuming folder names represent categories:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(load_files.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load text files with categories as subfolder names.\n",
        "\n",
        "    Individual samples are assumed to be files stored a two levels folder\n",
        "    structure such as the following:\n",
        "\n",
        "        container_folder/\n",
        "            category_1_folder/\n",
        "                file_1.txt\n",
        "                file_2.txt\n",
        "                ...\n",
        "                file_42.txt\n",
        "            category_2_folder/\n",
        "                file_43.txt\n",
        "                file_44.txt\n",
        "                ...\n",
        "\n",
        "    The folder names are used as supervised signal label names. The\n",
        "    individual file names are not important.\n",
        "\n",
        "    This function does not try to extract features into a numpy array or\n",
        "    scipy sparse matrix. In addition, if load_content is false it\n",
        "    does not try to load the files in memory.\n",
        "\n",
        "    To use text files in a scikit-learn classification or clustering\n",
        "    algorithm, you will need to use the `sklearn.feature_extraction.text`\n",
        "    module to build a feature extraction transformer that suits your\n",
        "    problem.\n",
        "\n",
        "    If you set load_content=True, you should also specify the encoding of\n",
        "    the text using the 'encoding' parameter. For many modern text files,\n",
        "    'utf-8' will be the correct encoding. If you leave encoding equal to None,\n",
        "    then the content will be made of bytes instead of Unicode, and you will\n",
        "    not be able to use most functions in `sklearn.feature_extraction.text`.\n",
        "\n",
        "    Similar feature extractors should be built for other kind of unstructured\n",
        "    data input such as images, audio, video, ...\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    container_path : string or unicode\n",
        "        Path to the main folder holding one subfolder per category\n",
        "\n",
        "    description: string or unicode, optional (default=None)\n",
        "        A paragraph describing the characteristic of the dataset: its source,\n",
        "        reference, etc.\n",
        "\n",
        "    categories : A collection of strings or None, optional (default=None)\n",
        "        If None (default), load all the categories.\n",
        "        If not None, list of category names to load (other categories ignored).\n",
        "\n",
        "    load_content : boolean, optional (default=True)\n",
        "        Whether to load or not the content of the different files. If\n",
        "        true a 'data' attribute containing the text information is present\n",
        "        in the data structure returned. If not, a filenames attribute\n",
        "        gives the path to the files.\n",
        "\n",
        "    encoding : string or None (default is None)\n",
        "        If None, do not try to decode the content of the files (e.g. for\n",
        "        images or other non-text content).\n",
        "        If not None, encoding to use to decode text files to Unicode if\n",
        "        load_content is True.\n",
        "\n",
        "    decode_error: {'strict', 'ignore', 'replace'}, optional\n",
        "        Instruction on what to do if a byte sequence is given to analyze that\n",
        "        contains characters not of the given `encoding`. Passed as keyword\n",
        "        argument 'errors' to bytes.decode.\n",
        "\n",
        "    shuffle : bool, optional (default=True)\n",
        "        Whether or not to shuffle the data: might be important for models that\n",
        "        make the assumption that the samples are independent and identically\n",
        "        distributed (i.i.d.), such as stochastic gradient descent.\n",
        "\n",
        "    random_state : int, RandomState instance or None, optional (default=0)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : Bunch\n",
        "        Dictionary-like object, the interesting attributes are: either\n",
        "        data, the raw text data to learn, or 'filenames', the files\n",
        "        holding it, 'target', the classification labels (integer index),\n",
        "        'target_names', the meaning of the labels, and 'DESCR', the full\n",
        "        description of the dataset.\n",
        "    \n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train = load_files('datasets/20news-bydate-train/',\n",
      "  encoding='latin-1', random_state=42)\n",
      "all_twenty_test = load_files('datasets/20news-bydate-test/',\n",
      "    encoding='latin-1', random_state=42)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train.keys()\n",
      "all_twenty_train.filenames"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "array(['datasets/20news-bydate-train/sci.electronics\\\\53898',\n",
        "       'datasets/20news-bydate-train/misc.forsale\\\\75862',\n",
        "       'datasets/20news-bydate-train/rec.sport.baseball\\\\104544', ...,\n",
        "       'datasets/20news-bydate-train/rec.sport.baseball\\\\104352',\n",
        "       'datasets/20news-bydate-train/comp.graphics\\\\38557',\n",
        "       'datasets/20news-bydate-train/sci.electronics\\\\53671'], \n",
        "      dtype='|S59')"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_target_names = all_twenty_train.target_names\n",
      "all_target_names"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "array([12,  6,  9, ...,  9,  1, 12])"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train.target.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "(11314L,)"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_test.target.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "(7532L,)"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(all_twenty_train.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "11314"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(all_twenty_train.data[0])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "unicode"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display_sample(i, dataset):\n",
      "    print(\"Class name: \" + dataset.target_names[dataset.target[i]])\n",
      "    print(\"Text content:\\n\")\n",
      "    print(dataset.data[i])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display_sample(0, all_twenty_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Class name: sci.electronics\n",
        "Text content:\n",
        "\n",
        "From: wtm@uhura.neoucom.edu (Bill Mayhew)\n",
        "Subject: Re: How to the disks copy protected.\n",
        "Organization: Northeastern Ohio Universities College of Medicine\n",
        "Lines: 23\n",
        "\n",
        "Write a good manual to go with the software.  The hassle of\n",
        "photocopying the manual is offset by simplicity of purchasing\n",
        "the package for only $15.  Also, consider offering an inexpensive\n",
        "but attractive perc for registered users.  For instance, a coffee\n",
        "mug.  You could produce and mail the incentive for a couple of\n",
        "dollars, so consider pricing the product at $17.95.\n",
        "\n",
        "You're lucky if only 20% of the instances of your program in use\n",
        "are non-licensed users.\n",
        "\n",
        "The best approach is to estimate your loss and accomodate that into\n",
        "your price structure.  Sure it hurts legitimate users, but too bad.\n",
        "Retailers have to charge off loss to shoplifters onto paying\n",
        "customers; the software industry is the same.\n",
        "\n",
        "Unless your product is exceptionally unique, using an ostensibly\n",
        "copy-proof disk will just send your customers to the competetion.\n",
        "\n",
        "\n",
        "-- \n",
        "Bill Mayhew      NEOUCOM Computer Services Department\n",
        "Rootstown, OH  44272-9995  USA    phone: 216-325-2511\n",
        "wtm@uhura.neoucom.edu (140.220.1.1)    146.580: N8WED\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display_sample(1, all_twenty_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Class name: misc.forsale\n",
        "Text content:\n",
        "\n",
        "From: andy@SAIL.Stanford.EDU (Andy Freeman)\n",
        "Subject: Re: Catalog of Hard-to-Find PC Enhancements (Repost)\n",
        "Organization: Computer Science Department,  Stanford University.\n",
        "Lines: 33\n",
        "\n",
        ">andy@SAIL.Stanford.EDU (Andy Freeman) writes:\n",
        ">> >In article <C5ELME.4z4@unix.portal.com> jdoll@shell.portal.com (Joe Doll) wr\n",
        ">> >>   \"The Catalog of Personal Computing Tools for Engineers and Scien-\n",
        ">> >>   tists\" lists hardware cards and application software packages for \n",
        ">> >>   PC/XT/AT/PS/2 class machines.  Focus is on engineering and scien-\n",
        ">> >>   tific applications of PCs, such as data acquisition/control, \n",
        ">> >>   design automation, and data analysis and presentation.  \n",
        ">> >\n",
        ">> >>   If you would like a free copy, reply with your (U. S. Postal) \n",
        ">> >>   mailing address.\n",
        ">> \n",
        ">> Don't bother - it never comes.  It's a cheap trick for building a\n",
        ">> mailing list to sell if my junk mail flow is any indication.\n",
        ">> \n",
        ">> -andy sent his address months ago\n",
        ">\n",
        ">Perhaps we can get Portal to nuke this weasal.  I never received a \n",
        ">catalog either.  If that person doesn't respond to a growing flame, then \n",
        ">we can assume that we'yall look forward to lotsa junk mail.\n",
        "\n",
        "I don't want him nuked, I want him to be honest.  The junk mail has\n",
        "been much more interesting than the promised catalog.  If I'd known\n",
        "what I was going to get, I wouldn't have hesitated.  I wouldn't be\n",
        "surprised if there were other folks who looked at the ad and said\n",
        "\"nope\" but who would be very interested in the junk mail that results.\n",
        "Similarly, there are people who wanted the advertised catalog who\n",
        "aren't happy with the junk they got instead.\n",
        "\n",
        "The folks buying the mailing lists would prefer an honest ad, and\n",
        "so would the people reading it.\n",
        "\n",
        "-andy\n",
        "--\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's compute the (uncompressed, in-memory) size of the training and test sets in MB assuming an 8-bit encoding (in this case, all chars can be encoded using the latin-1 charset)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_size(text, charset='iso-8859-1'):\n",
      "    return len(text.encode(charset)) * 8 * 1e-6\n",
      "\n",
      "train_size_mb = sum(text_size(text) for text in all_twenty_train.data) \n",
      "test_size_mb = sum(text_size(text) for text in all_twenty_test.data)\n",
      "\n",
      "print(\"Training set size: {0} MB\".format(int(train_size_mb)))\n",
      "print(\"Testing set size: {0} MB\".format(int(test_size_mb)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set size: 176 MB\n",
        "Testing set size: 110 MB\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If we only consider a small subset of the 4 categories selected from the initial example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_small_size_mb = sum(text_size(text) for text in twenty_train_small.data) \n",
      "test_small_size_mb = sum(text_size(text) for text in twenty_test_small.data)\n",
      "\n",
      "print(\"Training set size: {0} MB\".format(int(train_small_size_mb)))\n",
      "print(\"Testing set size: {0} MB\".format(int(test_small_size_mb)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set size: 31 MB\n",
        "Testing set size: 22 MB\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Extracting Text Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Terms that occur in only a few documents are often more valuable than ones that occur in many \u2013 inverse document frequency (${IDF}_j$)\n",
      "* The more often a term occurs in a document, the more likely it is to be important for that document \u2013 term frequency (${TF}_{ij}$)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(min_df=1)\n",
      "\n",
      "%time X_train_small = vectorizer.fit_transform(twenty_train_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 993 ms\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The results is not a `numpy.array` but instead a `scipy.sparse` matrix. _(Similar to the DocumentTermMatrix in R's `tm` library.)_ This datastructure is quite similar to a 2D numpy array but it does not store the zeros."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_small"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "<2034x34118 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 323433 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "scipy.sparse matrices also have a shape attribute to access the dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = X_train_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This dataset has around 2000 samples (the rows of the data matrix):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "2034"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This is the same value as the number of strings in the original list of text documents:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(twenty_train_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "2034"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The columns represent the individual token occurrences:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "34118"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This number is the size of the vocabulary of the model extracted during fit in a Python dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "dict"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "34118"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The keys of the `vocabulary_` attribute are also called feature names and can be accessed as a list of strings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "34118"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Here are the first 10 elements (sorted in lexicographical order):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()[:10]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "[u'00',\n",
        " u'000',\n",
        " u'0000',\n",
        " u'00000',\n",
        " u'000000',\n",
        " u'000005102000',\n",
        " u'000021',\n",
        " u'000062david42',\n",
        " u'0000vec',\n",
        " u'0001']"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's have a look at the features from the middle:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()[n_features / 2:n_features / 2 + 10]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "[u'inadequate',\n",
        " u'inala',\n",
        " u'inalienable',\n",
        " u'inane',\n",
        " u'inanimate',\n",
        " u'inapplicable',\n",
        " u'inappropriate',\n",
        " u'inappropriately',\n",
        " u'inaudible',\n",
        " u'inbreeding']"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Training a Classifier on Text Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We have previously extracted a vector representation of the training corpus and put it into a variable name `X_train_small`. To train a supervised model, in this case a classifier, we also need "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_small = twenty_train_small.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "(2034L,)"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We can shape that we have the same number of samples for the input data and the labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_small.shape[0] == y_train_small.shape[0]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can now train a classifier, for instance a Multinomial Naive Bayesian classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "clf = MultinomialNB(alpha=0.1)\n",
      "clf"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X_train_small, y_train_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can now evaluate the classifier on the testing set. Let's first use the builtin score function, which is the rate of correct classification in the test set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test_small = vectorizer.transform(twenty_test_small.data)\n",
      "y_test_small = twenty_test_small.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "(1353, 34118)"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_test_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "(1353L,)"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test_small, y_test_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "0.89652623798965259"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We can also compute the score on the train set and observe that the model is both overfitting and underfitting a bit at the same time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_train_small, y_train_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "0.99262536873156337"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Alternative evaluation metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Na\u00efve Bayes is a probabilistic models: instead of just predicting a binary outcome (alt.atheism or talk.religion) given the input features it can also estimates the posterior probability of the outcome given the input features using the `predict_proba` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_predicted_proba = clf.predict_proba(X_test_small)\n",
      "target_predicted_proba[:5]\n",
      "pd.DataFrame(target_predicted_proba[:5], columns = twenty_train_small.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>alt.atheism</th>\n",
        "      <th>comp.graphics</th>\n",
        "      <th>sci.space</th>\n",
        "      <th>talk.religion.misc</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0.008570</td>\n",
        "      <td> 0.979264</td>\n",
        "      <td> 0.007399</td>\n",
        "      <td> 0.004767</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.000005</td>\n",
        "      <td> 0.000007</td>\n",
        "      <td> 0.999983</td>\n",
        "      <td> 0.000005</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.055389</td>\n",
        "      <td> 0.000046</td>\n",
        "      <td> 0.000036</td>\n",
        "      <td> 0.944528</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0.995390</td>\n",
        "      <td> 0.000033</td>\n",
        "      <td> 0.000034</td>\n",
        "      <td> 0.004543</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0.000026</td>\n",
        "      <td> 0.000196</td>\n",
        "      <td> 0.999649</td>\n",
        "      <td> 0.000129</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "   alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
        "0     0.008570       0.979264   0.007399            0.004767\n",
        "1     0.000005       0.000007   0.999983            0.000005\n",
        "2     0.055389       0.000046   0.000036            0.944528\n",
        "3     0.995390       0.000033   0.000034            0.004543\n",
        "4     0.000026       0.000196   0.999649            0.000129"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default the decision threshold is 0.5: if we vary the decision threshold from 0 to 1 we could generate a family of binary classifier models that address all the possible trade offs between false positive and false negative prediction errors.\n",
      "\n",
      "We can summarize the performance of a binary classifier for all the possible thresholds by plotting the ROC curves and quantifying the area under the curve (AUC):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_roc_curve(target_test, target_predicted_proba, categories):\n",
      "    from sklearn.metrics import roc_curve\n",
      "    from sklearn.metrics import auc\n",
      "    \n",
      "    for pos_label, category in enumerate(categories):\n",
      "        fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, pos_label], pos_label)\n",
      "        roc_auc = auc(fpr, tpr)\n",
      "        plt.plot(fpr, tpr, label='{} ROC curve (area = {:.3f})'.format(category, roc_auc))\n",
      "    \n",
      "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.0])\n",
      "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
      "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
      "    plt.title('Receiver Operating Characteristic')\n",
      "    plt.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_roc_curve(y_test_small, target_predicted_proba, twenty_test_small.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHcCAYAAACXot0HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nshBCCCGEFvAfWuVXWNOhB3cAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x17bb1d68>"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here the area under ROC curve ranges between .963 and .974. The ROC-AUC score of a random model is expected to 0.5 on average while the accuracy score of a random model depends on the class imbalance of the data. ROC-AUC can be seen as a way to callibrate the predictive accuracy of a model against class imbalance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introspecting the Behavior of the Text Vectorizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The text vectorizer has many parameters to customize it's behavior, in particular how it extracts tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(TfidfVectorizer.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "\n",
        "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : string {'filename', 'file', 'content'}\n",
        "        If 'filename', the sequence passed as an argument to fit is\n",
        "        expected to be a list of filenames that need reading to fetch\n",
        "        the raw content to analyze.\n",
        "\n",
        "        If 'file', the sequence items must have a 'read' method (file-like\n",
        "        object) that is called to fetch the bytes in memory.\n",
        "\n",
        "        Otherwise the input is expected to be the sequence strings or\n",
        "        bytes items are expected to be analyzed directly.\n",
        "\n",
        "    encoding : string, 'utf-8' by default.\n",
        "        If bytes or files are given to analyze, this encoding is used to\n",
        "        decode.\n",
        "\n",
        "    decode_error : {'strict', 'ignore', 'replace'}\n",
        "        Instruction on what to do if a byte sequence is given to analyze that\n",
        "        contains characters not of the given `encoding`. By default, it is\n",
        "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
        "        values are 'ignore' and 'replace'.\n",
        "\n",
        "    strip_accents : {'ascii', 'unicode', None}\n",
        "        Remove accents during the preprocessing step.\n",
        "        'ascii' is a fast method that only works on characters that have\n",
        "        an direct ASCII mapping.\n",
        "        'unicode' is a slightly slower method that works on any characters.\n",
        "        None (default) does nothing.\n",
        "\n",
        "    analyzer : string, {'word', 'char'} or callable\n",
        "        Whether the feature should be made of word or character n-grams.\n",
        "\n",
        "        If a callable is passed it is used to extract the sequence of features\n",
        "        out of the raw, unprocessed input.\n",
        "\n",
        "    preprocessor : callable or None (default)\n",
        "        Override the preprocessing (string transformation) stage while\n",
        "        preserving the tokenizing and n-grams generation steps.\n",
        "\n",
        "    tokenizer : callable or None (default)\n",
        "        Override the string tokenization step while preserving the\n",
        "        preprocessing and n-grams generation steps.\n",
        "\n",
        "    ngram_range : tuple (min_n, max_n)\n",
        "        The lower and upper boundary of the range of n-values for different\n",
        "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
        "        will be used.\n",
        "\n",
        "    stop_words : string {'english'}, list, or None (default)\n",
        "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
        "        list is returned. 'english' is currently the only supported string\n",
        "        value.\n",
        "\n",
        "        If a list, that list is assumed to contain stop words, all of which\n",
        "        will be removed from the resulting tokens.\n",
        "\n",
        "        If None, no stop words will be used. max_df can be set to a value\n",
        "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
        "        words based on intra corpus document frequency of terms.\n",
        "\n",
        "    lowercase : boolean, default True\n",
        "        Convert all characters to lowercase before tokenizing.\n",
        "\n",
        "    token_pattern : string\n",
        "        Regular expression denoting what constitutes a \"token\", only used\n",
        "        if `analyzer == 'word'`. The default regexp selects tokens of 2\n",
        "        or more alphanumeric characters (punctuation is completely ignored\n",
        "        and always treated as a token separator).\n",
        "\n",
        "    max_df : float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
        "        When building the vocabulary ignore terms that have a term frequency\n",
        "        strictly higher than the given threshold (corpus specific stop words).\n",
        "        If float, the parameter represents a proportion of documents, integer\n",
        "        absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    min_df : float in range [0.0, 1.0] or int, optional, 1 by default\n",
        "        When building the vocabulary ignore terms that have a term frequency\n",
        "        strictly lower than the given threshold.\n",
        "        This value is also called cut-off in the literature.\n",
        "        If float, the parameter represents a proportion of documents, integer\n",
        "        absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    max_features : optional, None by default\n",
        "        If not None, build a vocabulary that only consider the top\n",
        "        max_features ordered by term frequency across the corpus.\n",
        "\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    vocabulary : Mapping or iterable, optional\n",
        "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
        "        indices in the feature matrix, or an iterable over terms. If not\n",
        "        given, a vocabulary is determined from the input documents.\n",
        "\n",
        "    binary : boolean, False by default.\n",
        "        If True, all non-zero term counts are set to 1. This does not mean\n",
        "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
        "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
        "\n",
        "    dtype : type, optional\n",
        "        Type of the matrix returned by fit_transform() or transform().\n",
        "\n",
        "    norm : 'l1', 'l2' or None, optional\n",
        "        Norm used to normalize term vectors. None for no normalization.\n",
        "\n",
        "    use_idf : boolean, optional\n",
        "        Enable inverse-document-frequency reweighting.\n",
        "\n",
        "    smooth_idf : boolean, optional\n",
        "        Smooth idf weights by adding one to document frequencies, as if an\n",
        "        extra document was seen containing every term in the collection\n",
        "        exactly once. Prevents zero divisions.\n",
        "\n",
        "    sublinear_tf : boolean, optional\n",
        "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ``idf_`` : array, shape = [n_features], or None\n",
        "        The learned idf vector (global term weights)\n",
        "        when ``use_idf`` is set to True, None otherwise.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "    CountVectorizer\n",
        "        Tokenize the documents and count the occurrences of token and return\n",
        "        them as a sparse matrix\n",
        "\n",
        "    TfidfTransformer\n",
        "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
        "        sparse matrix of occurrence counts.\n",
        "\n",
        "    \n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The easiest way to introspect what the vectorizer is actually doing for a given test of parameters is call the `vectorizer.build_analyzer()` to get an instance of the text analyzer it uses to process the text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = TfidfVectorizer().build_analyzer()\n",
      "analyzer(\"I love scikit-learn: this is a cool Python lib!\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[u'love', u'scikit', u'learn', u'this', u'is', u'cool', u'python', u'lib']"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You can notice that all the tokens are lowercase, that the single letter word \"I\" was dropped, and that hyphenation is used. Let's change some of that default behavior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = TfidfVectorizer(\n",
      "    preprocessor=lambda text: text,  # disable lowercasing\n",
      "    token_pattern=ur'(?u)\\b[\\w-]+\\b', # treat hyphen as a letter\n",
      "                                      # do not exclude single letter tokens\n",
      ").build_analyzer()\n",
      "\n",
      "analyzer(\"I love scikit-learn: this is a cool Python lib!\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "[u'I',\n",
        " u'love',\n",
        " u'scikit-learn',\n",
        " u'this',\n",
        " u'is',\n",
        " u'a',\n",
        " u'cool',\n",
        " u'Python',\n",
        " u'lib']"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The analyzer name comes from the Lucene parlance: it wraps the sequential application of:\n",
      "\n",
      "- text preprocessing (processing the text documents as a whole, e.g. lowercasing)\n",
      "- text tokenization (splitting the document into a sequence of tokens)\n",
      "- token filtering and recombination (e.g. n-grams extraction, see later)\n",
      "\n",
      "The analyzer system of scikit-learn is much more basic than lucene's though."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**:\n",
      "\n",
      "- Write a pre-processor callable (e.g. a python function) to remove the headers of the text a newsgroup post.\n",
      "- Vectorize the data again and measure the impact on performance of removing the header info from the dataset.\n",
      "- Do you expect the performance of the model to improve or decrease? What is the score of a uniform random classifier on the same dataset?\n",
      "\n",
      "Hint: the `TfidfVectorizer` class can accept python functions to customize the `preprocessor`, `tokenizer` or `analyzer` stages of the vectorizer.\n",
      "    \n",
      "- type `TfidfVectorizer()` alone in a cell to see the default value of the parameters\n",
      "\n",
      "- type `TfidfVectorizer.__doc__` to print the constructor parameters doc or `?` suffix operator on a any Python class or method to read the docstring or even the `jQuery203031440189930439577_1424627223304` operator to read the source code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Solution**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "0.09950399197731058"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's write a Python function to strip the post headers and only retain the body (text after the first blank line):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_headers(post):\n",
      "    \"\"\"Find the first blank line and drop the headers to keep the body\"\"\"\n",
      "    if '\\n\\n' in post:\n",
      "        headers, body = post.split('\\n\\n', 1)\n",
      "        return body.lower()\n",
      "    else:\n",
      "        # Unexpected post inner-structure, be conservative\n",
      "        # and keep everything\n",
      "        return post.lower()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it on the first post. Here is the original post content, including the headers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "original_text = all_twenty_train.data[0]\n",
      "print(original_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: wtm@uhura.neoucom.edu (Bill Mayhew)\n",
        "Subject: Re: How to the disks copy protected.\n",
        "Organization: Northeastern Ohio Universities College of Medicine\n",
        "Lines: 23\n",
        "\n",
        "Write a good manual to go with the software.  The hassle of\n",
        "photocopying the manual is offset by simplicity of purchasing\n",
        "the package for only $15.  Also, consider offering an inexpensive\n",
        "but attractive perc for registered users.  For instance, a coffee\n",
        "mug.  You could produce and mail the incentive for a couple of\n",
        "dollars, so consider pricing the product at $17.95.\n",
        "\n",
        "You're lucky if only 20% of the instances of your program in use\n",
        "are non-licensed users.\n",
        "\n",
        "The best approach is to estimate your loss and accomodate that into\n",
        "your price structure.  Sure it hurts legitimate users, but too bad.\n",
        "Retailers have to charge off loss to shoplifters onto paying\n",
        "customers; the software industry is the same.\n",
        "\n",
        "Unless your product is exceptionally unique, using an ostensibly\n",
        "copy-proof disk will just send your customers to the competetion.\n",
        "\n",
        "\n",
        "-- \n",
        "Bill Mayhew      NEOUCOM Computer Services Department\n",
        "Rootstown, OH  44272-9995  USA    phone: 216-325-2511\n",
        "wtm@uhura.neoucom.edu (140.220.1.1)    146.580: N8WED\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is the result of applying our header stripping function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_body = strip_headers(original_text)\n",
      "print(text_body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "write a good manual to go with the software.  the hassle of\n",
        "photocopying the manual is offset by simplicity of purchasing\n",
        "the package for only $15.  also, consider offering an inexpensive\n",
        "but attractive perc for registered users.  for instance, a coffee\n",
        "mug.  you could produce and mail the incentive for a couple of\n",
        "dollars, so consider pricing the product at $17.95.\n",
        "\n",
        "you're lucky if only 20% of the instances of your program in use\n",
        "are non-licensed users.\n",
        "\n",
        "the best approach is to estimate your loss and accomodate that into\n",
        "your price structure.  sure it hurts legitimate users, but too bad.\n",
        "retailers have to charge off loss to shoplifters onto paying\n",
        "customers; the software industry is the same.\n",
        "\n",
        "unless your product is exceptionally unique, using an ostensibly\n",
        "copy-proof disk will just send your customers to the competetion.\n",
        "\n",
        "\n",
        "-- \n",
        "bill mayhew      neoucom computer services department\n",
        "rootstown, oh  44272-9995  usa    phone: 216-325-2511\n",
        "wtm@uhura.neoucom.edu (140.220.1.1)    146.580: n8wed\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's plug our function in the vectorizer and retrain a naive Bayes classifier (as done initially):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "strip_vectorizer = TfidfVectorizer(preprocessor=strip_headers, min_df=2)\n",
      "X_train_small_stripped = strip_vectorizer.fit_transform(\n",
      "    twenty_train_small.data)\n",
      "\n",
      "y_train_small_stripped = twenty_train_small.target\n",
      "\n",
      "classifier = MultinomialNB().fit(\n",
      "  X_train_small_stripped, y_train_small_stripped)\n",
      "\n",
      "plt.hist()\n",
      "\n",
      "print(\"Training score: {0:.1f}%\".format(\n",
      "    classifier.score(X_train_small_stripped, y_train_small_stripped) * 100))\n",
      "\n",
      "X_test_small_stripped = strip_vectorizer.transform(twenty_test_small.data)\n",
      "y_test_small_stripped = twenty_test_small.target\n",
      "print(\"Testing score: {0:.1f}%\".format(\n",
      "    classifier.score(X_test_small_stripped, y_test_small_stripped) * 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training score: 93.3%\n",
        "Testing score: 82.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So indeed the header data is making the problem easier (cheating one could say) but naive Bayes classifier can still guess 80% of the time against 1 / 4 == 25% mean score for a random guessing on the small subset with 4 target categories."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Selection of the Naive Bayes Classifier Parameter Alone"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The `MultinomialNB` class is a good baseline classifier for text as it's fast and has few parameters to tweak:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MultinomialNB()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(MultinomialNB.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    Naive Bayes classifier for multinomial models\n",
        "\n",
        "    The multinomial Naive Bayes classifier is suitable for classification with\n",
        "    discrete features (e.g., word counts for text classification). The\n",
        "    multinomial distribution normally requires integer feature counts. However,\n",
        "    in practice, fractional counts such as tf-idf may also work.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, optional (default=1.0)\n",
        "        Additive (Laplace/Lidstone) smoothing parameter\n",
        "        (0 for no smoothing).\n",
        "\n",
        "    fit_prior : boolean\n",
        "        Whether to learn class prior probabilities or not.\n",
        "        If false, a uniform prior will be used.\n",
        "\n",
        "    class_prior : array-like, size (n_classes,)\n",
        "        Prior probabilities of the classes. If specified the priors are not\n",
        "        adjusted according to the data.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `class_log_prior_` : array, shape (n_classes, )\n",
        "        Smoothed empirical log probability for each class.\n",
        "\n",
        "    `intercept_` : property\n",
        "        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
        "        as a linear model.\n",
        "\n",
        "    `feature_log_prob_`: array, shape (n_classes, n_features)\n",
        "        Empirical log probability of features\n",
        "        given a class, ``P(x_i|y)``.\n",
        "\n",
        "    `coef_` : property\n",
        "        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
        "        as a linear model.\n",
        "\n",
        "    `class_count_` : array, shape (n_classes,)\n",
        "        Number of samples encountered for each class during fitting. This\n",
        "        value is weighted by the sample weight when provided.\n",
        "\n",
        "    `feature_count_` : array, shape (n_classes, n_features)\n",
        "        Number of samples encountered for each (class, feature)\n",
        "        during fitting. This value is weighted by the sample weight when\n",
        "        provided.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import numpy as np\n",
        "    >>> X = np.random.randint(5, size=(6, 100))\n",
        "    >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
        "    >>> from sklearn.naive_bayes import MultinomialNB\n",
        "    >>> clf = MultinomialNB()\n",
        "    >>> clf.fit(X, y)\n",
        "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "    >>> print(clf.predict(X[2]))\n",
        "    [3]\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
        "    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
        "    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
        "    Information Retrieval. Cambridge University Press, pp. 234-265.\n",
        "    http://nlp.stanford.edu/IR-book/html/htmledition/\n",
        "        naive-bayes-text-classification-1.html\n",
        "    \n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "By reading the doc we can see that the `alpha` parameter is a good candidate to adjust the model for the bias (underfitting) vs variance (overfitting) trade-off."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Setting Up a Pipeline for Cross Validation and Model Selection of the Feature Extraction parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The feature extraction class has many options to customize its behavior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(TfidfVectorizer.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "\n",
        "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : string {'filename', 'file', 'content'}\n",
        "        If 'filename', the sequence passed as an argument to fit is\n",
        "        expected to be a list of filenames that need reading to fetch\n",
        "        the raw content to analyze.\n",
        "\n",
        "        If 'file', the sequence items must have a 'read' method (file-like\n",
        "        object) that is called to fetch the bytes in memory.\n",
        "\n",
        "        Otherwise the input is expected to be the sequence strings or\n",
        "        bytes items are expected to be analyzed directly.\n",
        "\n",
        "    encoding : string, 'utf-8' by default.\n",
        "        If bytes or files are given to analyze, this encoding is used to\n",
        "        decode.\n",
        "\n",
        "    decode_error : {'strict', 'ignore', 'replace'}\n",
        "        Instruction on what to do if a byte sequence is given to analyze that\n",
        "        contains characters not of the given `encoding`. By default, it is\n",
        "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
        "        values are 'ignore' and 'replace'.\n",
        "\n",
        "    strip_accents : {'ascii', 'unicode', None}\n",
        "        Remove accents during the preprocessing step.\n",
        "        'ascii' is a fast method that only works on characters that have\n",
        "        an direct ASCII mapping.\n",
        "        'unicode' is a slightly slower method that works on any characters.\n",
        "        None (default) does nothing.\n",
        "\n",
        "    analyzer : string, {'word', 'char'} or callable\n",
        "        Whether the feature should be made of word or character n-grams.\n",
        "\n",
        "        If a callable is passed it is used to extract the sequence of features\n",
        "        out of the raw, unprocessed input.\n",
        "\n",
        "    preprocessor : callable or None (default)\n",
        "        Override the preprocessing (string transformation) stage while\n",
        "        preserving the tokenizing and n-grams generation steps.\n",
        "\n",
        "    tokenizer : callable or None (default)\n",
        "        Override the string tokenization step while preserving the\n",
        "        preprocessing and n-grams generation steps.\n",
        "\n",
        "    ngram_range : tuple (min_n, max_n)\n",
        "        The lower and upper boundary of the range of n-values for different\n",
        "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
        "        will be used.\n",
        "\n",
        "    stop_words : string {'english'}, list, or None (default)\n",
        "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
        "        list is returned. 'english' is currently the only supported string\n",
        "        value.\n",
        "\n",
        "        If a list, that list is assumed to contain stop words, all of which\n",
        "        will be removed from the resulting tokens.\n",
        "\n",
        "        If None, no stop words will be used. max_df can be set to a value\n",
        "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
        "        words based on intra corpus document frequency of terms.\n",
        "\n",
        "    lowercase : boolean, default True\n",
        "        Convert all characters to lowercase before tokenizing.\n",
        "\n",
        "    token_pattern : string\n",
        "        Regular expression denoting what constitutes a \"token\", only used\n",
        "        if `analyzer == 'word'`. The default regexp selects tokens of 2\n",
        "        or more alphanumeric characters (punctuation is completely ignored\n",
        "        and always treated as a token separator).\n",
        "\n",
        "    max_df : float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
        "        When building the vocabulary ignore terms that have a term frequency\n",
        "        strictly higher than the given threshold (corpus specific stop words).\n",
        "        If float, the parameter represents a proportion of documents, integer\n",
        "        absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    min_df : float in range [0.0, 1.0] or int, optional, 1 by default\n",
        "        When building the vocabulary ignore terms that have a term frequency\n",
        "        strictly lower than the given threshold.\n",
        "        This value is also called cut-off in the literature.\n",
        "        If float, the parameter represents a proportion of documents, integer\n",
        "        absolute counts.\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    max_features : optional, None by default\n",
        "        If not None, build a vocabulary that only consider the top\n",
        "        max_features ordered by term frequency across the corpus.\n",
        "\n",
        "        This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "    vocabulary : Mapping or iterable, optional\n",
        "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
        "        indices in the feature matrix, or an iterable over terms. If not\n",
        "        given, a vocabulary is determined from the input documents.\n",
        "\n",
        "    binary : boolean, False by default.\n",
        "        If True, all non-zero term counts are set to 1. This does not mean\n",
        "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
        "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
        "\n",
        "    dtype : type, optional\n",
        "        Type of the matrix returned by fit_transform() or transform().\n",
        "\n",
        "    norm : 'l1', 'l2' or None, optional\n",
        "        Norm used to normalize term vectors. None for no normalization.\n",
        "\n",
        "    use_idf : boolean, optional\n",
        "        Enable inverse-document-frequency reweighting.\n",
        "\n",
        "    smooth_idf : boolean, optional\n",
        "        Smooth idf weights by adding one to document frequencies, as if an\n",
        "        extra document was seen containing every term in the collection\n",
        "        exactly once. Prevents zero divisions.\n",
        "\n",
        "    sublinear_tf : boolean, optional\n",
        "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    ``idf_`` : array, shape = [n_features], or None\n",
        "        The learned idf vector (global term weights)\n",
        "        when ``use_idf`` is set to True, None otherwise.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "    CountVectorizer\n",
        "        Tokenize the documents and count the occurrences of token and return\n",
        "        them as a sparse matrix\n",
        "\n",
        "    TfidfTransformer\n",
        "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
        "        sparse matrix of occurrence counts.\n",
        "\n",
        "    \n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In order to evaluate the impact of the parameters of the feature extraction one can chain a configured feature extraction and classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer()),\n",
      "    ('clf', MultinomialNB()),\n",
      "))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Such a pipeline can then be cross validated or even grid searched:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from scipy.stats import sem\n",
      "\n",
      "scores = cross_val_score(pipeline, twenty_train_small.data,\n",
      "                         twenty_train_small.target, cv=3, n_jobs=3)\n",
      "scores.mean(), sem(scores)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "(0.87461982902109792, 0.0040823205976758233)"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {\n",
      "    'vec__max_df': [0.8, 1.0],\n",
      "    'vec__ngram_range': [(1, 1), (1, 2)],\n",
      "    'clf__alpha': np.logspace(-5, 0, 6)\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=3)\n",
      "_ = gs.fit(twenty_train_small.data, twenty_train_small.target)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=3)]: Done   1 jobs       | elapsed:    4.7s\n",
        "[Parallel(n_jobs=3)]: Done  41 jobs       | elapsed:  2.0min\n",
        "[Parallel(n_jobs=3)]: Done  68 out of  72 | elapsed:  3.4min remaining:   11.8s\n",
        "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:  3.6min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   4.6s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   4.5s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   4.5s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  11.3s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  11.5s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  11.6s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   3.1s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   3.2s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1e-05 -   3.2s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  13.6s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  13.9s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=1e-05 -  14.1s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.4s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.4s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.7s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  14.2s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  14.3s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  14.5s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 .....\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.4s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.2s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.0001 -   3.0s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 .....\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  13.5s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  14.3s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.0001 -  14.7s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 -   3.8s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 -   3.3s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.001 -   3.4s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 ......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 -  11.0s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 -  11.8s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.001 -  11.7s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 -   4.4s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 -   4.3s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.001 -   4.0s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 ......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 ......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 -  12.0s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 -  12.0s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.001 -  12.2s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 .......[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 .......[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 .......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 -   3.9s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 -   4.6s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.01 -   5.2s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 .......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 .......[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 .......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 -  13.7s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 -  12.7s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.01 -  12.4s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 .......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 .......[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 .......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 -   3.1s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 -   3.8s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.01 -   4.3s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 .......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 .......[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 .......\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 -  15.5s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 -  15.4s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.01 -  14.9s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 -   3.2s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 -   3.1s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=0.1 -   3.6s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 -  15.3s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 -  15.3s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=0.1 -  15.7s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 ........[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 ........[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 -   3.4s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 -   4.2s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 -  15.7s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 ........[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=0.1 -   3.9s[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 -  16.9s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 -   4.8s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1.0 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=1.0, vec__ngram_range=(1, 2), clf__alpha=0.1 -  17.0s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 -   3.4s[CV]  vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1.0 -  13.3s\n",
        "\n",
        "\n",
        "[CV] vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 ........[CV] vec__max_df=0.8, vec__ngram_range=(1, 2), clf__alpha=1.0 ........[CV] vec__max_df=1.0, vec__ngram_range=(1, 1), clf__alpha=1.0 ........\n",
        "\n",
        "\n",
        "[CV]  vec__max_df=0.8, vec__ngram_range=(1, 1), clf__alpha=1.0 -   3.8s"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.best_score_"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "0.96361848574237952"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.best_params_"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "{'clf__alpha': 0.01, 'vec__max_df': 0.8, 'vec__ngram_range': (1, 2)}"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introspecting Model Performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Displaying the Most Discriminative Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's fit a model on the small dataset and collect info on the fitted components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n",
      "    ('clf', MultinomialNB(alpha = 0.001)),\n",
      "))\n",
      "_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec_name, vec = pipeline.steps[0]\n",
      "clf_name, clf = pipeline.steps[1]\n",
      "\n",
      "feature_names = vec.get_feature_names()\n",
      "target_names = twenty_train_small.target_names\n",
      "\n",
      "feature_weights = clf.coef_\n",
      "\n",
      "feature_weights.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "(4, 279569)"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "By sorting the feature weights on the linear model and asking the vectorizer what their names is, one can get a clue on what the model did actually learn on the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display_important_features(feature_names, target_names, weights, n_top=30):\n",
      "    for i, target_name in enumerate(target_names):\n",
      "        print(u\"Class: \" + target_name)\n",
      "        print(u\"\")\n",
      "        \n",
      "        sorted_features_indices = weights[i].argsort()[::-1]\n",
      "        \n",
      "        most_important = sorted_features_indices[:n_top]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in most_important))\n",
      "        print(u\"...\")\n",
      "        \n",
      "        least_important = sorted_features_indices[-n_top:]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in least_important))\n",
      "        print(u\"\")\n",
      "        \n",
      "display_important_features(feature_names, target_names, feature_weights)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Class: alt.atheism\n",
        "\n",
        "is: -6.0027, that: -6.0312, you: -6.2511, it: -6.3314, edu: -6.4635, not: -6.5979, keith: -6.6737, are: -6.7286, god: -6.7621, be: -6.7679, this: -6.8508, have: -6.8571, as: -6.9286, for: -6.9742, com: -6.9941, was: -7.0117, if: -7.0161, what: -7.0188, caltech edu: -7.0239, caltech: -7.0395, but: -7.0456, we: -7.0484, they: -7.0532, or: -7.0633, your: -7.0776, an: -7.0856, he: -7.0863, people: -7.1103, one: -7.1105, do: -7.1638\n",
        "...\n",
        "occultic writings: -15.9805, occurred close: -15.9805, occurred long: -15.9805, ocean of: -15.9805, occurs program: -15.9805, ocean marvin: -15.9805, ocean mariner: -15.9805, ocean in: -15.9805, ocean dynamics: -15.9805, ocean dt: -15.9805, ocean and: -15.9805, ocean aleph: -15.9805, ocean after: -15.9805, ocean: -15.9805, occurs under: -15.9805, occurs this: -15.9805, occurs now: -15.9805, occurred think: -15.9805, occurs may: -15.9805, occurs in: -15.9805, occurs even: -15.9805, occurs during: -15.9805, occurs at: -15.9805, occurs as: -15.9805, occurs after: -15.9805, occurring in: -15.9805, occurring: -15.9805, occurrence in: -15.9805, occurrence based: -15.9805, laboratory nasa: -15.9805\n",
        "\n",
        "Class: comp.graphics\n",
        "\n",
        "for: -6.5232, is: -6.5246, it: -6.5456, graphics: -6.5550, edu: -6.6665, that: -6.9367, you: -6.9517, on: -6.9593, this: -7.0023, have: -7.0823, or: -7.1226, can: -7.1328, any: -7.1529, image: -7.1651, be: -7.1743, university: -7.1775, with: -7.1914, com: -7.2013, files: -7.2251, thanks: -7.2414, if: -7.2704, there: -7.3163, 3d: -7.3234, file: -7.3337, are: -7.3455, but: -7.3655, posting: -7.3922, university of: -7.3987, windows: -7.4041, host: -7.4349"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "...\n",
        "occured: -15.9061, occurred to: -15.9061, occurrence: -15.9061, ocean in: -15.9061, occurs in: -15.9061, ocean dynamics: -15.9061, ocean and: -15.9061, ocean aleph: -15.9061, ocean after: -15.9061, occurs whenever: -15.9061, occurs when: -15.9061, occurs under: -15.9061, occurs or: -15.9061, occurs of: -15.9061, occurs now: -15.9061, occurs may: -15.9061, occurs even: -15.9061, occurrence based: -15.9061, occurs during: -15.9061, occurs at: -15.9061, occurs and: -15.9061, occurs after: -15.9061, occurring in: -15.9061, occurring: -15.9061, occurrences who: -15.9061, occurrences it: -15.9061, occurrences: -15.9061, occurrence of: -15.9061, occurrence in: -15.9061, laboratory nasa: -15.9061\n",
        "\n",
        "Class: sci.space\n",
        "\n",
        "space: -6.2911, is: -6.5869, that: -6.6500, it: -6.6725, edu: -6.7050, nasa: -6.7105, for: -6.8113, on: -6.8681, you: -6.9553, be: -6.9736, was: -7.0381, this: -7.0560, henry: -7.0744, of the: -7.1096, access: -7.1658, gov: -7.1702, com: -7.2087, as: -7.2198, at: -7.2211, alaska: -7.2383, are: -7.2405, they: -7.2748, toronto: -7.2751, would: -7.2767, moon: -7.2871, have: -7.3009, not: -7.3185, nasa gov: -7.3204, digex: -7.3577, access digex: -7.3577"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "...\n",
        "magnificent church: -16.1294, magney cco: -16.1294, magney: -16.1294, unicorns don: -16.1294, magos: -16.1294, magos is: -16.1294, mahood: -16.1294, magu: -16.1294, unicorn if: -16.1294, unicorn nott: -16.1294, mahdi or: -16.1294, mahdi is: -16.1294, mahdi: -16.1294, maharaj sawan: -16.1294, maharaj gurinder: -16.1294, maharaj: -16.1294, mahanta for: -16.1294, mahanta: -16.1294, mahabharata the: -16.1294, mahabharata: -16.1294, magus sang: -16.1294, magus in: -16.1294, magus during: -16.1294, magus: -16.1294, magupati head: -16.1294, magupati: -16.1294, magu rendered: -16.1294, magu has: -16.1294, magu and: -16.1294, \u00band the: -16.1294\n",
        "\n",
        "Class: talk.religion.misc\n",
        "\n",
        "that: -6.1157, is: -6.2229, you: -6.3350, it: -6.6493, not: -6.6993, god: -6.8269, com: -6.8471, he: -6.8576, are: -6.8596, edu: -6.8870, sandvik: -6.8977, of the: -6.9046, as: -6.9459, this: -6.9646, jesus: -6.9714, for: -6.9743, be: -6.9748, they: -7.0126, have: -7.0412, was: -7.0674, with: -7.0833, we: -7.0838, who: -7.0931, what: -7.1481, your: -7.1744, his: -7.2007, christian: -7.2177, on: -7.2337, by: -7.2348, or: -7.2370\n",
        "...\n",
        "nature but: -15.7612, nature bill: -15.7612, nature produces: -15.7612, nature religions: -15.7612, nature second: -15.7612, nature simplicity: -15.7612, naught liberating: -15.7612, naught: -15.7612, nau edu: -15.7612, nau: -15.7612, natuur en: -15.7612, natuur: -15.7612, natures of: -15.7612, natures: -15.7612, natured the: -15.7612, natured: -15.7612, nature you: -15.7612, nature yet: -15.7612, nature yes: -15.7612, nature which: -15.7612, nature very: -15.7612, nature ve: -15.7612, nature v324: -15.7612, nature undecided: -15.7612, nature uncorruptable: -15.7612, nature to: -15.7612, nature thus: -15.7612, nature think: -15.7612, nature the: -15.7612, laboratory nasa: -15.7612\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Displaying the per-class Classification Reports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "predicted = pipeline.predict(twenty_test_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(classification_report(twenty_test_small.target, predicted,\n",
      "                            target_names=twenty_test_small.target_names))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                    precision    recall  f1-score   support\n",
        "\n",
        "       alt.atheism       0.83      0.88      0.86       319\n",
        "     comp.graphics       0.94      0.95      0.94       389\n",
        "         sci.space       0.94      0.95      0.94       394\n",
        "talk.religion.misc       0.86      0.76      0.81       251\n",
        "\n",
        "       avg / total       0.90      0.90      0.90      1353\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Printing the Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "The confusion matrix summarize which class where by having a look at off-diagonal entries: here we can see that articles about atheism have been wrongly classified as being about religion 57 times for instance: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "pd.DataFrame(confusion_matrix(twenty_test_small.target, predicted), \n",
      "             index = pd.MultiIndex.from_product([['actual'], twenty_test_small.target_names]),\n",
      "             columns = pd.MultiIndex.from_product([['predicted'], twenty_test_small.target_names]))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th colspan=\"4\" halign=\"left\">predicted</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>alt.atheism</th>\n",
        "      <th>comp.graphics</th>\n",
        "      <th>sci.space</th>\n",
        "      <th>talk.religion.misc</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"4\" valign=\"top\">actual</th>\n",
        "      <th>alt.atheism</th>\n",
        "      <td> 282</td>\n",
        "      <td>   3</td>\n",
        "      <td>   5</td>\n",
        "      <td>  29</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comp.graphics</th>\n",
        "      <td>   7</td>\n",
        "      <td> 368</td>\n",
        "      <td>  13</td>\n",
        "      <td>   1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>sci.space</th>\n",
        "      <td>   3</td>\n",
        "      <td>  15</td>\n",
        "      <td> 375</td>\n",
        "      <td>   1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>talk.religion.misc</th>\n",
        "      <td>  48</td>\n",
        "      <td>   4</td>\n",
        "      <td>   7</td>\n",
        "      <td> 192</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "                            predicted                          \\\n",
        "                          alt.atheism comp.graphics sci.space   \n",
        "actual alt.atheism                282             3         5   \n",
        "       comp.graphics                7           368        13   \n",
        "       sci.space                    3            15       375   \n",
        "       talk.religion.misc          48             4         7   \n",
        "\n",
        "                                              \n",
        "                          talk.religion.misc  \n",
        "actual alt.atheism                        29  \n",
        "       comp.graphics                       1  \n",
        "       sci.space                           1  \n",
        "       talk.religion.misc                192  "
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Pairs\n",
      "=========\n",
      "\n",
      "```bash\n",
      "$ unzip Classification_data -d Classification_data\n",
      "```\n",
      "\n",
      "1. Load the dateset using `load_files` (hint: our `categories` are now `spam`, `easy_ham`, etc.)\n",
      "2. Write a pre-processor callable to remove the message headers.\n",
      "3. Set up a pipeline for cross validation and model selection using `spam` and `easy_ham`.\n",
      "    - Which parameters should be optimized?\n",
      "    - Do you expect the results to be different from the parameters above? Why/why not?\n",
      "    - Are there other parameters we should optimize that we haven't tested?\n",
      "4. Use `GridSearchCV` to find optimal parameters for vectorizor and classifier.\n",
      "5. Run classifier against `hard_ham`. What percentage of `hard_ham` does it correctly identify as not `spam`?\n",
      "6. Display the most discriminative features. Anything stick out?\n",
      "7. Run classifier against `spam_2`, `easy_ham_2`, `hard_ham_2`. \n",
      "    - Plot the ROC curve (along with AUC) for each case. \n",
      "    - Print the confusion matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_files\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "# Load the text data\n",
      "# categories = [\n",
      "#     'spam',\n",
      "#     'hard_ham',\n",
      "#     'easy_ham'\n",
      "# ]\n",
      "train = load_files('Classification_data/data0/',\n",
      "    categories=['spam', 'easy_ham'], encoding='latin-1')\n",
      "\n",
      "test = load_files('Classification_data/data0/',\n",
      "    categories=['hard_ham'], encoding='latin-1')\n",
      "\n",
      "train.target_names, test.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "(['easy_ham', 'spam'], ['hard_ham'])"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From spamassassin-talk-admin@lists.sourceforge.net  Wed Jul 24 17:21:42 2002\n",
        "Return-Path: <spamassassin-talk-admin@example.sourceforge.net>\n",
        "Delivered-To: yyyy@localhost.netnoteinc.com\n",
        "Received: from localhost (localhost [127.0.0.1])\n",
        "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id B2C7B440A8\n",
        "\tfor <jm@localhost>; Wed, 24 Jul 2002 12:21:40 -0400 (EDT)\n",
        "Received: from dogma.slashnull.org [212.17.35.15]\n",
        "\tby localhost with IMAP (fetchmail-5.9.0)\n",
        "\tfor jm@localhost (single-drop); Wed, 24 Jul 2002 17:21:40 +0100 (IST)\n",
        "Received: from usw-sf-list2.sourceforge.net (usw-sf-fw2.sourceforge.net\n",
        "    [216.136.171.252]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\n",
        "    g6OGIB425651 for <jm-sa@jmason.org>; Wed, 24 Jul 2002 17:18:11 +0100\n",
        "Received: from usw-sf-list1-b.sourceforge.net ([10.3.1.13]\n",
        "    helo=usw-sf-list1.sourceforge.net) by usw-sf-list2.sourceforge.net with\n",
        "    esmtp (Exim 3.31-VA-mm2 #1 (Debian)) id 17XOo6-0004Bo-00; Wed,\n",
        "    24 Jul 2002 09:16:10 -0700\n",
        "Received: from anti.cnc.bc.ca ([142.27.70.181] helo=anti) by\n",
        "    usw-sf-list1.sourceforge.net with smtp (Exim 3.31-VA-mm2 #1 (Debian)) id\n",
        "    17XOnX-0003WJ-00 for <spamassassin-talk@lists.sourceforge.net>;\n",
        "    Wed, 24 Jul 2002 09:15:35 -0700\n",
        "Received: FROM cnc.bc.ca BY anti ; Wed Jul 24 09:25:40 2002 -0700\n",
        "Received: from cnc.bc.ca ([142.27.70.153]) by cnc.bc.ca ; Wed,\n",
        "    24 Jul 2002 09:15:33 -0800 gmt\n",
        "Message-Id: <3D3ED2B9.866CC459@cnc.bc.ca>\n",
        "From: Kevin Gagel <gagel@cnc.bc.ca>\n",
        "Reply-To: gagel@cnc.bc.ca\n",
        "Organization: College of New Caledonia\n",
        "X-Mailer: Mozilla 4.79 [en] (Windows NT 5.0; U)\n",
        "X-Accept-Language: en\n",
        "MIME-Version: 1.0\n",
        "To: Theo Van Dinter <felicity@kluge.net>,\n",
        "\t\"spamassassin-talk@lists.sourceforge.net\" <spamassassin-talk@lists.sourceforge.net>\n",
        "Subject: Re: [SAtalk] Spam through - insanefunnies - clairification of X-Spam-Status fi elds?\n",
        "References: <F9102D41F595D311ACA7009027DE2C8404E11AF0@c3po.heurikon.com>\n",
        "    <20020724152341.GE24673@kluge.net>\n",
        "Content-Type: text/plain; charset=us-ascii\n",
        "Content-Transfer-Encoding: 7bit\n",
        "X-Dsmtpfooter: true\n",
        "Sender: spamassassin-talk-admin@example.sourceforge.net\n",
        "Errors-To: spamassassin-talk-admin@example.sourceforge.net\n",
        "X-Beenthere: spamassassin-talk@example.sourceforge.net\n",
        "X-Mailman-Version: 2.0.9-sf.net\n",
        "Precedence: bulk\n",
        "List-Help: <mailto:spamassassin-talk-request@example.sourceforge.net?subject=help>\n",
        "List-Post: <mailto:spamassassin-talk@example.sourceforge.net>\n",
        "List-Subscribe: <https://example.sourceforge.net/lists/listinfo/spamassassin-talk>,\n",
        "    <mailto:spamassassin-talk-request@lists.sourceforge.net?subject=subscribe>\n",
        "List-Id: Talk about SpamAssassin <spamassassin-talk.example.sourceforge.net>\n",
        "List-Unsubscribe: <https://example.sourceforge.net/lists/listinfo/spamassassin-talk>,\n",
        "    <mailto:spamassassin-talk-request@lists.sourceforge.net?subject=unsubscribe>\n",
        "List-Archive: <http://www.geocrawler.com/redir-sf.php3?list=spamassassin-talk>\n",
        "X-Original-Date: Wed, 24 Jul 2002 09:15:53 -0700\n",
        "Date: Wed, 24 Jul 2002 09:15:53 -0700\n",
        "\n",
        "Amis-v or there is another prefs file that SA is using. I had a heck of a time\n",
        "figuring out where to find my site wide file because of my configuration.\n",
        "\n",
        "If your using spamd and you want your users to have some control using\n",
        "user_prefs then check their ~/spamassassin file.\n",
        "If your using spamd and you have a site wide only policy then make sure that\n",
        "spamd is started with the -x option.\n",
        "If you used the -x option then the only place that it should get the rules from\n",
        "would be from the local.cf in the /etc/mail/spamassassin directory. Assuming a\n",
        "default install.\n",
        "\n",
        "Theo Van Dinter wrote:\n",
        "> \n",
        "> On Wed, Jul 24, 2002 at 10:18:28AM -0500, Stewart, John wrote:\n",
        "> > X-Virus-Scanned: by amavisd-new amavisd-new-20020630\n",
        "> > X-Spam-Status: No, hits=6.5 tagged_above=5.1 required=6.9 tests=PLING,\n",
        "> > MONEY_BACK, CLICK_BELOW, POR\n",
        "> > N_14, CLICK_HERE_LINK, FREQ_SPAM_PHRASE\n",
        "> > X-Razor-id: d92173a8dfc60567e55efcf6bf264fd7f7a7369a\n",
        "> >\n",
        "> > Doesn't hits=6.5 mean that it should be tagged as spam? Why the\n",
        "> > X-Spam-Status of no then?\n",
        "> \n",
        "> required is 6.9, it only scored 6.5, so it's not spam according to SA.\n",
        "> \n",
        "> > Why is required=6.9 if I have required_hits at 5 in the local.cf? Where the\n",
        "> > heck does that number come from?\n",
        "> >\n",
        "> > Also, what is tagged_above=? I cannot find any information about it on the\n",
        "> > SpamAssassin site.\n",
        "> \n",
        "> Good questions...  \"tagged_above\" doesn't appear anywhere in SA (at\n",
        "> least according to `find`).  I would guess it's amavis doing some\n",
        "> hacking around.\n",
        "> \n",
        "> --\n",
        "> Randomly Generated Tagline:\n",
        "> D'oh!  English!  Who needs that?  I'm never going to England. Come on,\n",
        ">  let's smoke.\n",
        "> \n",
        ">                 -- Homer Simpson, talking Barney into cutting class\n",
        ">                    The Way We Was\n",
        "> \n",
        "> -------------------------------------------------------\n",
        "> This sf.net email is sponsored by:ThinkGeek\n",
        "> Welcome to geek heaven.\n",
        "> http://thinkgeek.com/sf\n",
        "> _______________________________________________\n",
        "> Spamassassin-talk mailing list\n",
        "> Spamassassin-talk@lists.sourceforge.net\n",
        "> https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
        "\n",
        "-- \n",
        "========================\n",
        "Kevin W. Gagel\n",
        "Network Administrator\n",
        "College of New Caledonia\n",
        "gagel@cnc.bc.ca\n",
        "postmaster@cnc.bc.ca\n",
        "(250)562-2131 loc. 448\n",
        "========================\n",
        "\n",
        "--------------------------------\n",
        "The College of New Caledonia    \n",
        "Visit us at http://www.cnc.bc.ca\n",
        "--------------------------------\n",
        "\n",
        "\n",
        "-------------------------------------------------------\n",
        "This sf.net email is sponsored by:ThinkGeek\n",
        "Welcome to geek heaven.\n",
        "http://thinkgeek.com/sf\n",
        "_______________________________________________\n",
        "Spamassassin-talk mailing list\n",
        "Spamassassin-talk@lists.sourceforge.net\n",
        "https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_headers(post):\n",
      "    \"\"\"Find the first blank line and drop the headers to keep the body\"\"\"\n",
      "    if '\\n\\n' in post:\n",
      "        headers, body = post.split('\\n\\n', 1)\n",
      "        return body.lower()\n",
      "    else:\n",
      "        # Unexpected post inner-structure, be conservative\n",
      "        # and keep everything\n",
      "        return post.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print strip_headers(train.data[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "amis-v or there is another prefs file that sa is using. i had a heck of a time\n",
        "figuring out where to find my site wide file because of my configuration.\n",
        "\n",
        "if your using spamd and you want your users to have some control using\n",
        "user_prefs then check their ~/spamassassin file.\n",
        "if your using spamd and you have a site wide only policy then make sure that\n",
        "spamd is started with the -x option.\n",
        "if you used the -x option then the only place that it should get the rules from\n",
        "would be from the local.cf in the /etc/mail/spamassassin directory. assuming a\n",
        "default install.\n",
        "\n",
        "theo van dinter wrote:\n",
        "> \n",
        "> on wed, jul 24, 2002 at 10:18:28am -0500, stewart, john wrote:\n",
        "> > x-virus-scanned: by amavisd-new amavisd-new-20020630\n",
        "> > x-spam-status: no, hits=6.5 tagged_above=5.1 required=6.9 tests=pling,\n",
        "> > money_back, click_below, por\n",
        "> > n_14, click_here_link, freq_spam_phrase\n",
        "> > x-razor-id: d92173a8dfc60567e55efcf6bf264fd7f7a7369a\n",
        "> >\n",
        "> > doesn't hits=6.5 mean that it should be tagged as spam? why the\n",
        "> > x-spam-status of no then?\n",
        "> \n",
        "> required is 6.9, it only scored 6.5, so it's not spam according to sa.\n",
        "> \n",
        "> > why is required=6.9 if i have required_hits at 5 in the local.cf? where the\n",
        "> > heck does that number come from?\n",
        "> >\n",
        "> > also, what is tagged_above=? i cannot find any information about it on the\n",
        "> > spamassassin site.\n",
        "> \n",
        "> good questions...  \"tagged_above\" doesn't appear anywhere in sa (at\n",
        "> least according to `find`).  i would guess it's amavis doing some\n",
        "> hacking around.\n",
        "> \n",
        "> --\n",
        "> randomly generated tagline:\n",
        "> d'oh!  english!  who needs that?  i'm never going to england. come on,\n",
        ">  let's smoke.\n",
        "> \n",
        ">                 -- homer simpson, talking barney into cutting class\n",
        ">                    the way we was\n",
        "> \n",
        "> -------------------------------------------------------\n",
        "> this sf.net email is sponsored by:thinkgeek\n",
        "> welcome to geek heaven.\n",
        "> http://thinkgeek.com/sf\n",
        "> _______________________________________________\n",
        "> spamassassin-talk mailing list\n",
        "> spamassassin-talk@lists.sourceforge.net\n",
        "> https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
        "\n",
        "-- \n",
        "========================\n",
        "kevin w. gagel\n",
        "network administrator\n",
        "college of new caledonia\n",
        "gagel@cnc.bc.ca\n",
        "postmaster@cnc.bc.ca\n",
        "(250)562-2131 loc. 448\n",
        "========================\n",
        "\n",
        "--------------------------------\n",
        "the college of new caledonia    \n",
        "visit us at http://www.cnc.bc.ca\n",
        "--------------------------------\n",
        "\n",
        "\n",
        "-------------------------------------------------------\n",
        "this sf.net email is sponsored by:thinkgeek\n",
        "welcome to geek heaven.\n",
        "http://thinkgeek.com/sf\n",
        "_______________________________________________\n",
        "spamassassin-talk mailing list\n",
        "spamassassin-talk@lists.sourceforge.net\n",
        "https://lists.sourceforge.net/lists/listinfo/spamassassin-talk\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_html(post):\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer(preprocessor = strip_headers, encoding = 'latin-1')),\n",
      "    ('clf', MultinomialNB()),\n",
      "))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectorizer: min_df, max_df, stop_words, n_gram_range, \n",
      "MNB: alpha"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(train.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "2799"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {\n",
      "    # 'vec__min_df': np.linspace(1, 9, 5),\n",
      "    'vec__max_df': [0.7, 1.0],\n",
      "    'vec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
      "    'clf__alpha': np.logspace(-5, 0, 6)\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=3)\n",
      "_ = gs.fit(train.data, train.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gs.best_params_\n",
      "print gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'vec__max_df': 0.7, 'vec__ngram_range': (2, 2), 'clf__alpha': 1.0000000000000001e-05}\n",
        "0.994640943194\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline2 = Pipeline((\n",
      "    ('vec', TfidfVectorizer(preprocessor = strip_headers, encoding = 'latin-1', max_df = 0.7, ngram_range = (2, 2))),\n",
      "    ('clf', MultinomialNB(alpha = 0.00001)),\n",
      "))\n",
      "\n",
      "_ = pipeline2.fit(train.data, train.target)\n",
      "preds = pipeline2.predict(test.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "print(classification_report(test.target, preds,\n",
      "                            target_names=train.target_names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "   easy_ham       1.00      0.24      0.38       248\n",
        "       spam       0.00      0.00      0.00         0\n",
        "\n",
        "avg / total       1.00      0.24      0.38       248\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\David\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "import pandas as pd\n",
      "\n",
      "pd.DataFrame(confusion_matrix(test.target, preds), \n",
      "             index = pd.MultiIndex.from_product([['actual'], ['ham', 'null']]),\n",
      "             columns = pd.MultiIndex.from_product([['predicted'], train.target_names]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>easy_ham</th>\n",
        "      <th>spam</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
        "      <th>ham</th>\n",
        "      <td> 59</td>\n",
        "      <td> 189</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>null</th>\n",
        "      <td>  0</td>\n",
        "      <td>   0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "             predicted      \n",
        "              easy_ham  spam\n",
        "actual ham          59   189\n",
        "       null          0     0"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "59/(59+189.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.23790322580645162"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec_name, vec = pipeline2.steps[0]\n",
      "clf_name, clf = pipeline2.steps[1]\n",
      "\n",
      "feature_names = vec.get_feature_names()\n",
      "target_names = train.target_names\n",
      "\n",
      "feature_weights = clf.coef_\n",
      "\n",
      "feature_weights.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(1L, 319282L)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display_important_features(feature_names, target_names, weights, n_top=30):\n",
      "    for i, target_name in enumerate(target_names):\n",
      "        print(u\"Class: \" + target_name)\n",
      "        print(u\"\")\n",
      "        \n",
      "        sorted_features_indices = weights[i].argsort()[::-1]\n",
      "        \n",
      "        most_important = sorted_features_indices[:n_top]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in most_important))\n",
      "        print(u\"...\")\n",
      "        \n",
      "        least_important = sorted_features_indices[-n_top:]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in least_important))\n",
      "        print(u\"\")\n",
      "        \n",
      "display_important_features(feature_names, target_names, feature_weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Class: easy_ham\n",
        "\n",
        "br br: -5.9316, font face: -5.9325, font font: -6.1052, font size: -6.2234, color 3d: -6.2383, nbsp nbsp: -6.2789, td tr: -6.2998, size 3d: -6.3151, face 3d: -6.3737, width 3d: -6.3960, font color: -6.4436, tr td: -6.4998, arial helvetica: -6.5478, face arial: -6.5879, sans serif: -6.6577, align 3d: -6.6759, http www: -6.6854, helvetica sans: -6.6964, font td: -6.7251, td width: -6.7360, td td: -6.7668, 3d http: -6.7716, 3d arial: -6.8406, align center: -6.8449, tr tr: -6.8539, href http: -6.8639, br font: -6.8785, 3d center: -7.0002, click here: -7.0459, face verdana: -7.0464\n",
        "...\n",
        "du dirigeant: -21.3404, post by: -21.3404, post clue: -21.3404, dtlogin screen: -21.3404, post company: -21.3404, post daily: -21.3404, post description: -21.3404, post em: -21.3404, post etc: -21.3404, dteste et: -21.3404, dtek chalmers: -21.3404, post apocalyptic: -21.3404, post about: -21.3404, du2n9fwsc9jp0pps1 tg9nf06pt5nqd1efui9tp39ut3ratwcd: -21.3404, possibly producing: -21.3404, possibly not: -21.3404, possibly other: -21.3404, possibly posted: -21.3404, dub foundation: -21.3404, duane reade: -21.3404, dual system: -21.3404, possibly replace: -21.3404, du5zbqvzc2ndbqvnqostn9 h1ifmjl: -21.3404, dual port: -21.3404, dual pii350: -21.3404, possibly saturday: -21.3404, dual boot: -21.3404, dual benefit: -21.3404, du9xh0jgwe9zxwjvsod wv8tt10fxi9fp2bp7ppb4pdhz2ls: -21.3404, institutions this: -21.3404\n",
        "\n",
        "Class: spam\n",
        "\n"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "index 1 is out of bounds for axis 0 with size 1",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-30e7677f0d21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdisplay_important_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-13-30e7677f0d21>\u001b[0m in \u001b[0;36mdisplay_important_features\u001b[1;34m(feature_names, target_names, weights, n_top)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msorted_features_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mmost_important\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted_features_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_top\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "train1 = load_files('Classification_data/data1/',\n",
      "    categories=['spam', 'easy_ham'], encoding='latin-1')\n",
      "\n",
      "test1 = load_files('Classification_data/data1/',\n",
      "    categories=['hard_ham'], encoding='latin-1')\n",
      "\n",
      "train1.target_names, test1.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "(['easy_ham', 'spam'], ['hard_ham'])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = pipeline2.fit(train1.data, train1.target)\n",
      "preds = pipeline2.predict(test1.data)\n",
      "pred_proba = pipeline2.predict_proba(test1.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_roc_curve(target_test, target_predicted_proba, categories):\n",
      "    from sklearn.metrics import roc_curve\n",
      "    from sklearn.metrics import auc\n",
      "    \n",
      "    for pos_label, category in enumerate(categories):\n",
      "        fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, pos_label], pos_label)\n",
      "        roc_auc = auc(fpr, tpr)\n",
      "        plt.plot(fpr, tpr, label='{} ROC curve (area = {:.3f})'.format(category, roc_auc))\n",
      "    \n",
      "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.0])\n",
      "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
      "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
      "    plt.title('Receiver Operating Characteristic')\n",
      "    plt.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from seaborn import plt\n",
      "\n",
      "plot_roc_curve(preds, pred_proba, ???)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhjcrDEYIwxJgdLDMYYY3L4f/CLwO+i/jYdAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x2000ba20>"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On Your Own\n",
      "========\n",
      "\n",
      "1. Read the Na\u00efve Bayes documentation at [scikit-learn.org](http://scikit-learn.org/stable/modules/naive_bayes.html). There are three Na\u00efve Bayes classifiers described. Which of the other two might also be appropriate for this task?\n",
      "2. Explain your choice and apply it to either the spam/ham dataset (if you completed the pair assignment) or the newsgroups dataset (if you didn't).\n",
      "3. Use grid search cross validation to find the best parameters for both the vectorizor and classifier.\n",
      "    - Do different parameters for the vectorizor work better for this classifier?\n",
      "4. Does this classifier do better or worse than the multinomial classifier?\n",
      "5. Advanced: consider the descriptions of the two classifiers in light of which does better for this problem. Can you posit a theory as to why one classifier should do better than the other?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}