{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In depth with SVMs: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM stands for \"support vector machines\". They are efficient and easy to use estimators.\n",
    "They come in two kinds: SVCs, Support Vector Classifiers, for classification problems, and SVRs, Support Vector Regressors, for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVMs: some intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop our intuitions, let us look at a very simple classification problem: classifying irises based on sepal length and width. We only use 2 features to enable easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear')\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the prediction, we evaluate it on a grid of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_estimator(estimator, X, y):\n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_estimator(svc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `kernel=\"linear\"` gives linear decision frontiers: the frontier between two classes is a line.\n",
    "\n",
    "How does multi-class work? With the `SVC` object, it is done by combining \"one versus one\" decisions on each pair of classes.\n",
    "\n",
    "**LinearSVC**: for linear kernels, there is another object, the `LinearSVC` that uses a different algorithm. On some data it may be faster (for instance sparse data, as in text mining). It uses a \"one versus all\" strategy for multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_estimator(svm.LinearSVC(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVRs (Support Vector Regression) work like SVCs, but for regression rather than classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vectors and regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support vectors**: The way a support vector machine works is by finding a decision boundary separating the 2 classes that is spanned by a small number of training samples, called \"support vectors\". These samples lie closest to the other class, and can thus be considered as most representative samples in terms of the two-class discrimination problem.\n",
    "\n",
    "To make visualization even simpler, let us consider a 2 class problem, for instance using classes 1 and 2 in the iris dataset. These 2 classes are not well linearly separable, which makes it an interesting problem.\n",
    "\n",
    "The indices of the support vectors for each class can be found in the `support_vectors_` attribute. We highlight them in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])]\n",
    "plot_estimator(svc, X, y)\n",
    "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization**: Considering only the discriminant samples is a form of regularization. Indeed, it forces the model to be simpler in how it combines observed structures.\n",
    "\n",
    "This regularization can be tuned with the *C* parameter:\n",
    "\n",
    "- Low C values: many support vectors... Decision frontier = mean(class A) - mean(class B)\n",
    "- High C values: small number of support vectors: Decision frontier fully driven by most disriminant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_hiC = svm.SVC(kernel='linear', C=1e3)  # C = 1,000\n",
    "plot_estimator(svc_hiC, X, y)\n",
    "plt.scatter(svc_hiC.support_vectors_[:, 0], svc_hiC.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)\n",
    "plt.title('High C values: small number ({:,}) of support vectors'.format(len(svc_hiC.support_vectors_)))\n",
    "\n",
    "svc_loC = svm.SVC(kernel='linear', C=1e-3)  # C = 0.001\n",
    "plot_estimator(svc_loC, X, y)\n",
    "plt.scatter(svc_loC.support_vectors_[:, 0], svc_loC.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)\n",
    "plt.title('Low C values: high number ({:,}) of support vectors'.format(len(svc_loC.support_vectors_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice features of SVMs is that on many datasets, the default value 'C=1' works well.\n",
    "\n",
    "**Practical note: Normalizing data** For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is often important to get good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One appealling aspect of SVMs is that they can easily be used to build non linear decision frontiers using **kernels**. Kernel define the 'building blocks' that are assembled to form a decision rule.\n",
    "\n",
    "- **linear** will give linear decision frontiers. It is the most computationally efficient approach and the one that requires the least amount of data.\n",
    "\n",
    "- **poly** will give decision frontiers that are polynomial. The order of this polynomial is given by the 'order' argument.\n",
    "\n",
    "- **rbf** uses 'radial basis functions' centered at each support vector to assemble a decision frontier. The size of the RBFs, that ultimately controls the smmothness of the decision frontier. RBFs are the most flexible approach, but also the one that will require the largest amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear')\n",
    "plot_estimator(svc, X, y)\n",
    "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)\n",
    "plt.title('Linear kernel')\n",
    "\n",
    "svc = svm.SVC(kernel='poly', degree=4)\n",
    "plot_estimator(svc, X, y)\n",
    "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)\n",
    "plt.title('Polynomial kernel')\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', gamma=1e2)\n",
    "plot_estimator(svc, X, y)\n",
    "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, facecolors='none', zorder=10)\n",
    "plt.title('RBF kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that RBFs and more flexible and fit our train data best. Remember, minimizing train error is not a goal per se, and we have to watch for overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: tune an SVM on the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "#... now all that is left to do is the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Example from Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll take a look at a simple facial recognition example.\n",
    "This uses a dataset available within scikit-learn consisting of a\n",
    "subset of the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
    "data.  Note that this is a relatively large download (~200MB) so it may\n",
    "take a while to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4,\n",
    "                                       data_home='datasets')\n",
    "lfw_people.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're on a unix-based system such as linux or Mac OSX, these shell commands\n",
    "can be used to see the downloaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh datasets/lfw_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these faces to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "# plot several images\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(lfw_people.images[i], cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "\n",
    "unique_targets = np.unique(lfw_people.target)\n",
    "counts = [(lfw_people.target == i).sum() for i in unique_targets]\n",
    "\n",
    "plt.xticks(unique_targets, lfw_people.target_names[unique_targets])\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45, size=14)\n",
    "_ = plt.bar(unique_targets, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that these faces have already been localized and scaled\n",
    "to a common size.  This is an important preprocessing piece for facial\n",
    "recognition, and is a process that can require a large collection of training\n",
    "data.  This can be done in scikit-learn, but the challenge is gathering a\n",
    "sufficient amount of training data for the algorithm to work\n",
    "\n",
    "Fortunately, this piece is common enough that it has been done.  One good\n",
    "resource is [OpenCV](http://opencv.willowgarage.com/wiki/FaceRecognition), the\n",
    "*Open Computer Vision Library*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform a Support Vector classification of the images.  We'll\n",
    "do a typical train-test split on the images to make this happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target, random_state=0)\n",
    "\n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1850 dimensions is a lot for SVM.  We can use PCA to reduce these 1850 features to a manageable\n",
    "size (say, 150), while maintaining most of the information in the dataset.  Here it is useful to use a variant\n",
    "of PCA called ``RandomizedPCA``, which is an approximation of PCA that can be much faster for large\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting part of PCA is that it computes the \"mean\" face, which can be\n",
    "interesting to examine using `plt.imshow`:\n",
    "```python\n",
    "plt.imshow(pca.mean_.reshape((50, 37)), cmap=plt.cm.bone)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components measure deviations about this mean along orthogonal axes.\n",
    "It is also interesting to visualize these principal components:\n",
    "\n",
    "*hint: notice how we used* `fig.add_subplot` *above to plot a grid of faces. Do the same to plot the* PCA `components_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components (\"eigenfaces\") are ordered by their importance from top-left to bottom-right.\n",
    "We see that the first few components seem to primarily take care of lighting\n",
    "conditions; the remaining components pull out certain identifying features:\n",
    "the nose, eyes, eyebrows, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this projection computed, we can now project our original training\n",
    "and test data onto the PCA basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These projected components correspond to factors in a linear combination of\n",
    "component images such that the combination approaches the original face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the Learning: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform support-vector-machine classification on this reduced dataset. Start with:\n",
    "```python\n",
    "svm.SVC(C=5., gamma=0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate how well this classification did.  First, we might plot a\n",
    "few of the test-cases with the labels learned from the training set. \n",
    "Putting something like the following in your `for` loop might help:\n",
    "\n",
    "```python\n",
    "color = 'black' if y_pred == y_test[i] else 'red'\n",
    "ax.set_title(lfw_people.target_names[y_pred], fontsize='small', color=color)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is correct on an impressive number of images given the simplicity\n",
    "of its learning model!  Using a linear classifier on 150 features derived from\n",
    "the pixel-level data, the algorithm correctly identifies a large number of the\n",
    "people in the images.\n",
    "\n",
    "Again, we can\n",
    "quantify this effectiveness using one of several measures from the ``sklearn.metrics``\n",
    "module.  First we can do the classification report, which shows the precision,\n",
    "recall and other measures of the \"goodness\" of the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.classification_report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting metric is the *confusion matrix*, which indicates how often\n",
    "any two items are mixed-up.  The confusion matrix of a perfect classifier\n",
    "would only have nonzero entries on the diagonal, with zeros on the off-diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.f1_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used PCA as a pre-processing step before applying our support vector machine classifier.\n",
    "Plugging the output of one estimator directly into the input of a second estimator is a commonly\n",
    "used pattern; for this reason scikit-learn provides a ``Pipeline`` object which automates this\n",
    "process.  The above problem can be re-expressed as a pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not identical because we used the randomized version of the PCA -- because the\n",
    "projection varies slightly each time, the results vary slightly as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Note on Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used PCA \"eigenfaces\" as a pre-processing step for facial recognition.\n",
    "The reason we chose this is because PCA is a broadly-applicable technique, which can\n",
    "be useful for a wide array of data types.  Research in the field of facial recognition\n",
    "in particular, however, has shown that other more specific feature extraction methods\n",
    "are can be much more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search:\n",
    "\n",
    "Use grid search to determine\n",
    "\n",
    "1. If it is better to `whiten` or not.\n",
    "2. Whether the `rbf`, `linear`, or `poly` kernel is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
