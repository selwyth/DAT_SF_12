{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Support Vector Machines\n",
    "========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Previously:\n",
    "--------------------------------------------------------\n",
    "\n",
    "- SciKit-Learn\n",
    "    - Training a predictive model on numerical features\n",
    "    - Model evaluation and interpretation\n",
    "    - Cross-validation\n",
    "    - More feature engineering and richer models\n",
    "- Decision Trees\n",
    "    - Building Decision Trees\n",
    "    - Optimization Functions\n",
    "    - Preventing Overfit\n",
    "    - Implementing Decision Trees with Scikit-Learn\n",
    "\n",
    "### Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Agenda\n",
    "--------------------------------------------------------\n",
    "\n",
    "1. Theory\n",
    "    1. Intro to Support Vector Machines (SVMs)\n",
    "    2. Maximum Margin Hyperplanes\n",
    "    3. Slack Variables\n",
    "    4. Nonlinear Classification with Kernels\n",
    "2. Practice\n",
    "    1. Cross-Validation\n",
    "    2. Model Selection with Grid Search\n",
    "    3. Learning Curves and the Bias-Variance trade-off\n",
    "    4. Final Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a support vector machine?\n",
    "--------------------------------------------------------\n",
    "Support vector machine is an algorithm that generates a linear binary classification using geometric functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Goal: Explicity constructed to minimize generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Binary**: Solves a two-class problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Linear**: creates linear decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How is the decision boundary derived?\n",
    "--------------------------------------------------------\n",
    "SVMs use geometric reasoning to determine classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More importantly, they use the geometric concept of **margin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Geometry break: margins\n",
    "--------------------------------------------------------\n",
    "Here, margins mean the region along the decision boundary that is free of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Margins provide the most impact in SVMs--even moving *only one point* on the margin can completely change the decision boundary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Margin: Example\n",
    "--------------------------------------------------------\n",
    "<img src=\"assets/margin_01.png\" width=\"600\" />\n",
    "<small>source: [Data Analysis with Open Source Tools](http://shop.oreilly.com/product/9780596802363.do), by Philipp K. Janert. O’Reilly Media, 2011.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The goal of an SVM is to create the linear decision boundary with the largest margin. This is commonly called the **maximum margin hyperplane**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A *hyperplane* is just a high-dimensional generalization of a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If SVM is a linear classifier, how can you use it for nonlinear classification?\n",
    "--------------------------------------------------------\n",
    "Using a clever maneuver called the **kernel trick**.  \n",
    "<small>(more on that later)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonlinear applications of SVM rely on an implicit (nonlinear) mapping $\\Phi$ that sends vectors from the original feature space $K$ into a higher-dimensional feature space $K'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nonlinear classification in $K$ is then obtained by creating a linear decision boundary in $K'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practice, this involves no computations in the higher dimensional space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Margin Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's our goal for optimization?\n",
    "--------------------------------------------------------\n",
    "SVM will always solve for a decision boundary that minimizes generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the same as finding a boundary that has the greatest margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wider the margin, clearer the distinction between two classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Margin: Example\n",
    "--------------------------------------------------------\n",
    "<img src=\"assets/margin_01.png\" width=\"600\" />\n",
    "<small>source: [Data Analysis with Open Source Tools](http://shop.oreilly.com/product/9780596802363.do), by Philipp K. Janert. O’Reilly Media, 2011.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hyperplanes and Support Vectors\n",
    "--------------------------------------------------------\n",
    "To optimize a support vector machine, it should be able to determine the maximum margin hyperplane (mmh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that the margin depends only on a *subset* of the training data; namely, those points that are nearest to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These points are called the **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The other points (far from the decision boundary) don’t affect the construction of the mmh at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### All of the decision boundaries we’ve seen so far have split the data perfectly; e.g. the data are *linearly separable*, and therefore the training error is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that...\n",
    "--------------------------------------------------------\n",
    "We are always solving for one optimum (global, instead of local optima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If our data (like in the previous examples) is linearly seperable, the training error is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However... when is data ever really like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need an approach to improve data where this is not the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimizing Large Margin: A Review\n",
    "--------------------------------------------------------\n",
    "SVMs will always optimize using Support Vectors and hyperplanes\n",
    "\n",
    "Support Vectors represent the margin data points where classification is less clear\n",
    "\n",
    "With the best hyperplane, we get the best, large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Slack Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that in building the hard margin classifier, we assumed that our data was **linearly separable** (eg, that we could perfectly classify each record with a linear decision boundary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose that this was not true, or suppose that we wanted to use a larger margin at the expense of incurring some training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be done using by introducing **slack variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Slack variables $\\xi$ generalize the optimization problem to permit some misclassified training records (which come at a cost $C$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Result: **Soft Margin Classifier**. Attempts to split data as cleanly as possible, maximizing the margin of correctly classified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This an example of *bias-variance tradeoff*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example of soft margins\n",
    "--------------------------------------------------------\n",
    "<img src=\"assets/svm_soft_margins.png\" width=\"600\" />  \n",
    "The effect of the soft-margin constant, $C$, on the decision boundary. A smaller value of $C$ (right) allows us to ignore points close to the boundary, and increases the margin. The decision boundary between negative examples (red circles) and positive examples (blue crosses) is shown as a thick line. The lighter lines are on the margin (discriminant value equal to -1 or +1). The grayscale level represents the value of the discriminant function, dark for low values and a light shade for high values. \n",
    "\n",
    "<small>source: [pyml.sourceforge.net/doc/howto.pdf](http://pyml.sourceforge.net/doc/howto.pdf)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Selecting slack variables\n",
    "--------------------------------------------------------\n",
    "Often, the optimization of slack variables comes between exponentially growing sequences of $C$\n",
    "\n",
    "Higher values of variable $C =$ higher accuracy in model\n",
    "\n",
    "Lower values of $C =$ training error and better generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Review: Slack Variables\n",
    "--------------------------------------------------------\n",
    "Slack variables allow for inaccuracies in classification for a more generalized, accurate model\n",
    "\n",
    "Soft Margins are created as a result of generalization variable C\n",
    "\n",
    "We trade off accuracy for generalization by picking a lower value C to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nonlinear Classification with Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonlinear Classification\n",
    "--------------------------------------------------------\n",
    "Suppose we need a more complex classifier than a linear decision boundary allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One possibility is to add nonlinear combinations of features to the data, and then to create a linear decision boundary in the enhanced (higher-dimensional) feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This linear decision boundary will be mapped to a nonlinear decision boundary in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: working with non-linear classification\n",
    "--------------------------------------------------------\n",
    "<img src=\"assets/nonlinear_svm_01.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonlinear Classification\n",
    "--------------------------------------------------------\n",
    "The logic of this approach is sound, but there are a few problems with this version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, this will not scale well, since it requires many high-dimensional calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It will likely lead to more complexity (both modeling complexity and computational complexity) than we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonlinear Classification\n",
    "--------------------------------------------------------\n",
    "Let’s hang on to the logic of the previous example, namely:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- remap the feature vectors $x_i$ into a higher-dimensional space $K'$\n",
    "- create a linear decision boundary in $K'$\n",
    "- back out the nonlinear decision boundary in $K$ from the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But we want to save ourselves the trouble of doing a lot of additional high-dimensional calculations. How can we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kernel Trick\n",
    "--------------------------------------------------------\n",
    "Instead, we use **kernel functions** that map two vectors in a higher-dimensional feature space $K'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The upshot is that we can use a kernel function to *implicitly* train our model in a higher-dimensional feature space, *without* incurring additional computational complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As long as the kernel function satisfies certain conditions, our conclusions above regarding the mmh continue to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These conditions are contained in a result called _Mercer’s theorem_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words, no algorithmic changes are necessary, and all the benefits of a linear SVM are maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some Popular Kernels:\n",
    "--------------------------------------------------------\n",
    "\n",
    "Linear Kernel: $$ k(x,x') = \\langle x,x' \\rangle $$\n",
    "\n",
    "Polynomial Kernel: $$ k(x,x') = (x^\\top x'+1)^d $$\n",
    "\n",
    "Gaussian Kernel: $$ k(x,x') = exp(-\\gamma||x-x'||^2) $$\n",
    "\n",
    "Radial Basis Kernel: $$ k(x,x') = exp(-\\frac{||x-x'||^2_2}{2\\sigma^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **hyperparameters** $d, \\gamma$ affect the flexibility of the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Linear & Polynomial Kernels\n",
    "--------------------------------------------------------\n",
    "<img src=\"assets/nonlinear_svm_02.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kernels: Review\n",
    "--------------------------------------------------------\n",
    "Kernel methods are powerful and popular techniques that can produce accurate results\n",
    "\n",
    "Computationally efficient ways to deal with non linear classification problems\n",
    "\n",
    "Beware: entering dangerous territory here: tread lightly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SVMs (and **kernel methods** in general) are versatile, powerful, and popular techniques that can produce accurate results for a wide array of classification problems.\n",
    "\n",
    "The main disadvantage of SVMs is the lack of intuition they produce. These models are truly black boxes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "===\n",
    "```bash\n",
    "python svm_gui.py\n",
    "```\n",
    "[16-svm_lab.ipynb](16-svm_lab.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
